{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, I create the citation vector dataframe for each year for section H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "import pickle\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data_drct = '/home/somayeh/Documents/Career/Data_Incubator/Capstone/Data/'\n",
    "result_drct = '/home/somayeh/Documents/Career/Data_Incubator/Capstone/Result/'\n",
    "filename1 = 'application.tsv'\n",
    "filename2 = 'cpc_current.tsv'\n",
    "filename3 = 'uspatentcitation.tsv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "patent issuance year is obtained in \"Patent_query-Oct10-Copy1.ipynb, and stored in:\n",
    "\n",
    "/home/somayeh/Documents/Career/Data_Incubator/Project/patent_yr\n",
    "\n",
    "Here, I need the patent_id, citation_id, and year. For that, I am creating another (previous one is redundant) set of yearly dataframes containing all those information (as below):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_chunk = pd.read_csv(data_drct+filename2,sep='\\t',header=0, chunksize=1000000)\n",
    "patent_list2 = []  \n",
    "for cnt, chunk in enumerate(df_chunk):\n",
    "#     print(cnt)    \n",
    "    chunk = chunk[chunk[\"section_id\"] == \"H\"]\n",
    "    chunk = chunk[[\"patent_id\",\"group_id\",\"category\",\"sequence\"]]\n",
    "    chunk[\"category\"] = pd.Categorical(chunk[\"category\"])\n",
    "    chunk[\"patent_id\"] = chunk[\"patent_id\"].astype(int)\n",
    "    patent_list2.append(chunk)\n",
    "    \n",
    "patent_id_group = pd.concat(patent_list2)\n",
    "del patent_list2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patent_id_group.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(patent_id_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patent_id_group.to_pickle(result_drct+\"/patent_id_group.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del patent_id_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List all patents available after 1960 and their issuance year (to be used later here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_patent_year(yr):\n",
    "    drct = '/home/somayeh/Documents/Career/Data_Incubator/Project/'\n",
    "    fname = drct+'/patent_yr/'+str(yr)+'.pkl'\n",
    "    with open(fname, 'rb') as f:\n",
    "        df = pickle.load(f)\n",
    "    return(df)\n",
    "     \n",
    "# get list of patent ids stored in patent_id_group data file   \n",
    "fname = result_drct+\"/patent_id_group.pkl\"\n",
    "with open(fname, 'rb') as f:\n",
    "    df = pickle.load(f)\n",
    "patent_list1 = df.patent_id.unique()\n",
    "del df\n",
    "\n",
    "# filter citation ids and their year\n",
    "df_1 = pd.DataFrame()\n",
    "for yr in range(1960,2018):\n",
    "    print(yr)\n",
    "    df1 = get_patent_year(yr)   \n",
    "    df1 = df1[df1.citation_id.str.isnumeric()]\n",
    "    df1.citation_id = df1.citation_id.astype(int)\n",
    "    df1[df1.citation_id.isin(patent_list1)]\n",
    "    df_1 = pd.concat([df_1,df1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1.to_pickle(result_drct+\"/patent_id_year.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_1.citation_id.unique()), len(df_1.citation_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Reading patent groups from file:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = result_drct+\"/patent_id_group.pkl\"\n",
    "with open(fname, 'rb') as f:\n",
    "    patent_id_group = pickle.load(f)\n",
    "# patent_id_group = patent_id_group.drop_duplicates() \n",
    "patent_id_group = patent_id_group.sort_values(by=['patent_id', 'group_id'])\n",
    "patent_id_group = patent_id_group.drop(columns = [\"category\",\"sequence\"], axis=1)\n",
    "patent_id_group = patent_id_group.reset_index(drop=True)\n",
    "patent_id_group.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(patent_id_group.patent_id.unique()), len(patent_id_group.patent_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading patent ids and issuance year:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_per_year(yr):\n",
    "    \n",
    "    drct = '/home/somayeh/Documents/Career/Data_Incubator/Project/'\n",
    "    fname = drct+'/patent_citation_yr/'+str(yr)+'.pkl'\n",
    "    with open(fname, 'rb') as f:\n",
    "        df = pickle.load(f)\n",
    "    df = (df\n",
    "            .drop(['patent_id','citation_id', 'year'], axis=1)\n",
    "            .join(df[['patent_id','citation_id', 'year']].apply(pd.to_numeric, errors='coerce')))\n",
    "    df = df.dropna()\n",
    "    df[['patent_id','citation_id']] = df[['patent_id','citation_id']].astype(int)\n",
    "    df = df.reset_index(drop=True)\n",
    "    \n",
    "    result_drct = '/home/somayeh/Documents/Career/Data_Incubator/Capstone/Result/'\n",
    "    fname = result_drct+\"/patent_id_group.pkl\"\n",
    "    with open(fname, 'rb') as f:\n",
    "        patent_id_group = pickle.load(f)\n",
    "    patent_id_group = patent_id_group[[\"patent_id\", \"group_id\"]]\n",
    "    patent_id_group = patent_id_group.loc[patent_id_group[\"patent_id\"].isin(df.patent_id)]\n",
    "\n",
    "    df = pd.merge(df, patent_id_group, how='left', on=['patent_id'])\n",
    "\n",
    "    df = df.rename(columns={'year': 'year_citation', 'group_id':'patent_group','patent_id': 'p_id', 'citation_id':'patent_id'})\n",
    "    df = pd.merge(df, patent_id_group, how='left', on=['patent_id'])\n",
    "    df = df.rename(columns={'p_id': 'patent_id', 'patent_id':'citation_id', 'group_id':'citation_group'})\n",
    "    df = df.dropna()\n",
    "    df = df.reset_index(drop=True)\n",
    "    del patent_id_group \n",
    "\n",
    "    result_drct = '/home/somayeh/Documents/Career/Data_Incubator/Capstone/Result/'\n",
    "    fname = result_drct+\"/patent_id_year.pkl\"\n",
    "    with open(fname, 'rb') as f:\n",
    "        df2 = pickle.load(f)\n",
    "    df2 = df2.loc[df2[\"year\"]>=yr-3]\n",
    "    df2[\"year\"] = df2[\"year\"].astype(int)\n",
    "    df2 = df2.rename(columns={'year': 'year_patent', 'citation_id':'patent_id'})\n",
    "    df = pd.merge(df, df2, how='left', on=['patent_id'])\n",
    "    df = df.dropna()\n",
    "    \n",
    "    \n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('H04L', 'H04L'), 2918),\n",
       " (('H04W', 'H04W'), 1409),\n",
       " (('H04W', 'H04L'), 1389),\n",
       " (('H04L', 'H04W'), 1286),\n",
       " (('H01L', 'H01L'), 889),\n",
       " (('H04N', 'H04N'), 855),\n",
       " (('H04L', 'H04M'), 635),\n",
       " (('H04M', 'H04L'), 632),\n",
       " (('H04W', 'H04M'), 601),\n",
       " (('H04M', 'H04W'), 539)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = get_df_per_year(2012)\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "all_links = list(zip(df.patent_group, df.citation_group))\n",
    "unique_links = set(list(zip(df.patent_group, df.citation_group)))\n",
    "\n",
    "dd = Counter(all_links)\n",
    "mc = dd.most_common(10)\n",
    "mc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List of all unique groups in H section\n",
    "# get list of group ids stored in patent_id_group data file   \n",
    "drct = '/home/somayeh/Documents/Career/Data_Incubator/Project/'\n",
    "fname = result_drct+\"/patent_id_group.pkl\"\n",
    "with open(fname, 'rb') as f:\n",
    "    df2 = pickle.load(f)\n",
    "group_list = df2.group_id.unique()\n",
    "del df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['H01J', 'H01L', 'H05F', 'H01H', 'H02J', 'H02P', 'H01S', 'H01C',\n",
       "       'H03K', 'H03C', 'H03G', 'H02N', 'H02K', 'H04B', 'H03H', 'H04L',\n",
       "       'H01F', 'H04W', 'H03M', 'H01Q', 'H02S', 'H03J', 'H03L', 'H03F',\n",
       "       'H05K', 'H01P', 'H04N', 'H04J', 'H01T', 'H01B', 'H01R', 'H01M',\n",
       "       'H05C', 'H02H', 'H05B', 'H01G', 'H02M', 'H04R', 'H05H', 'H02G',\n",
       "       'H02B', 'H04H', 'H04M', 'H04Q', 'H01K', 'H03B', 'H03D', 'H04S',\n",
       "       'H05G', 'H04K'], dtype=object)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22287, 567, 48)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_links), len(unique_links), len(df.patent_group.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_names, adj_idx = np.unique(all_links, return_inverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['H01B', 'H01C', 'H01F', 'H01G', 'H01H', 'H01J', 'H01L', 'H01M',\n",
       "        'H01P', 'H01Q', 'H01R', 'H01S', 'H01T', 'H02B', 'H02G', 'H02H',\n",
       "        'H02J', 'H02K', 'H02M', 'H02N', 'H02P', 'H02S', 'H03B', 'H03C',\n",
       "        'H03D', 'H03F', 'H03G', 'H03H', 'H03J', 'H03K', 'H03L', 'H03M',\n",
       "        'H04B', 'H04H', 'H04J', 'H04K', 'H04L', 'H04M', 'H04N', 'H04Q',\n",
       "        'H04R', 'H04S', 'H04W', 'H05B', 'H05F', 'H05G', 'H05H', 'H05K'],\n",
       "       dtype='<U4'), array([38, 38,  6, ..., 21, 16, 21]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_names, adj_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1.])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adj_idx = adj_idx.reshape(-1, 2)\n",
    "adj_matrix = np.zeros((len(node_names),)*2)\n",
    "adj_matrix[adj_idx[:, 0], adj_idx[:, 1]] = 1\n",
    "adj_matrix[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_df_per_year(1980)\n",
    "\n",
    "all_links = list(zip(df.patent_group, df.citation_group))\n",
    "unique_links = set(list(zip(df.patent_group, df.citation_group)))\n",
    "\n",
    "dd = Counter(all_links)\n",
    "mc = dd.most_common(50)\n",
    "mc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = df.patent_group.unique()\n",
    "All_Links = list(zip(df.patent_group, df.citation_group))\n",
    "\n",
    "G = nx.MultiGraph()\n",
    "G.add_edges_from(All_Links)\n",
    "M = nx.to_numpy_matrix(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import linalg as LA\n",
    "val, vec = LA.eig(M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = np.argmax(val)\n",
    "max(np.array(vec)[ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_df_per_year(2000)\n",
    "\n",
    "all_links = list(zip(df.patent_group, df.citation_group))\n",
    "unique_links = set(list(zip(df.patent_group, df.citation_group)))\n",
    "\n",
    "dd = Counter(all_links)\n",
    "mc = dd.most_common(100)\n",
    "mc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create network of citation for groups:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_df_per_year(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_graph(G):\n",
    "    pos=nx.spring_layout(G) # positions for all nodes\n",
    "    # nodes\n",
    "    nx.draw_networkx_nodes(G,pos, node_color='r',node_size=5,alpha=0.4)\n",
    "    #edges\n",
    "    nx.draw_networkx_edges(G,pos,width=1.0,alpha=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "nodes = df.patent_group.unique()\n",
    "Links = set(list(zip(df.patent_group, df.citation_group)))\n",
    "\n",
    "G = nx.Graph()  # initiate a graph\n",
    "G.add_nodes_from(list(nodes)) # add nodes to the graph. Here, the nodes are the unique authors\n",
    "G.add_edges_from(Links) # adding edge based on coauthorship\n",
    "\n",
    "\n",
    "remove = [node for node,degree in dict(G.degree()).items() if degree < 200]\n",
    "G.remove_nodes_from(remove)\n",
    "\n",
    "draw_graph(G)\n",
    "print(nx.info(G))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_graphs = nx.connected_component_subgraphs(G)\n",
    "len(sub_graphs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del G \n",
    "nodes = df.patent_group.unique()\n",
    "Links = list(zip(df.patent_group, df.citation_group))\n",
    "\n",
    "G2 = nx.MultiGraph()\n",
    "G2.add_nodes_from(list(nodes)) # add nodes to the graph. Here, the nodes are the unique authors\n",
    "G2.add_edges_from(Links) # adding edge based on coauthorship\n",
    "\n",
    "\n",
    "# remove = [node for node,degree in dict(G2.degree()).items() if degree < 50]\n",
    "# G2.remove_nodes_from(remove)\n",
    "\n",
    "# draw_graph(G)\n",
    "print(nx.info(G2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(Links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patent_citation_id_year = (patent_citation_id_year\n",
    "            .drop(['patent_id','citation_id', 'year'], axis=1)\n",
    "            .join(patent_citation_id_year[['patent_id','citation_id', 'year']].apply(pd.to_numeric, errors='coerce')))\n",
    "patent_citation_id_year[['patent_id','citation_id']] = patent_citation_id_year[['patent_id','citation_id']].fillna(0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patent_citation_id_year.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(patent_id_group), len(patent_id_group.patent_id.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p_group = []\n",
    "# for i in range(10):\n",
    "#     patent_id,citation_id, year = patent_citation_id_year.iloc[i]\n",
    "# #     print(i)\n",
    "#     if i>10:\n",
    "#         break\n",
    "#     p_group.append( patent_id_group.loc[patent_id_group.patent_id ==patent_id])\n",
    "# # print(patent_id, p_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = patent_citation_id_year.join(patent_id_group, on=['patent_id'], how='inner')\n",
    "patent_citation_id_year2 = pd.merge(patent_citation_id_year, patent_id_group, how='right', on=['patent_id'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(patent_citation_id_year2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = (result\n",
    "            .drop(['patent_id','citation_id', 'year'], axis=1)\n",
    "            .join(result[['patent_id','citation_id', 'year']].apply(pd.to_numeric, errors='coerce')))\n",
    "result[['patent_id','citation_id', 'year']] = result[['patent_id','citation_id', 'year']].fillna(0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(result.group_id.unique())\n",
    "len(result.citation_id.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patent_citation_id_year.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tmp = patent_citation_id_year.loc[patent_citation_id_year.patent_id.isin(patent_id_group.patent_id.astype(str))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tmp2= patent_citation_id_year.loc[patent_citation_id_year.citation_id.isin(patent_id_group.patent_id.astype(str))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(patent_citation_id_year), len(df_tmp),len(df_tmp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_tmp.patent_id.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tmp2 = patent_id_group.loc[patent_citation_id_year.patent_id.isin(patent_id_group.patent_id.astype(str))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patent_citation_id_year.patent_id[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patent_citation_id_year.patent_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patent_id_year = patent_id_year.drop_duplicates(subset='citation_id', keep = 'last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(patent_id_year), len(patent_id_year.citation_id.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the number distribution of patents over time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count =[]\n",
    "year = []\n",
    "for yr in range(1960,2018):\n",
    "    year.append(yr)\n",
    "    count.append(len(patent_id_year.loc[patent_id_year.year == yr]))\n",
    "plt.plot(year,count)\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Number of Issued Patents')\n",
    "# plt.legend( )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_chunk = pd.read_csv(drct+filename2,sep='\\t',header=0, chunksize=1000000)\n",
    "# patent_list2 = []  \n",
    "# for cnt, chunk in enumerate(df_chunk):\n",
    "#     print(cnt)    \n",
    "# #     chunk.drop(['patent_id','citation_id','uuid', 'name', 'kind','country','category','sequence'], axis=1)\n",
    "# #     chunk['year'] = pd.to_datetime(chunk['date'], format = '%Y-%M-%d', errors='coerce').dt.year\n",
    "#     patent_list2.append(chunk[['patent_id','group_id']])\n",
    "# #     if cnt>1:\n",
    "# #         break\n",
    "    \n",
    "# patent_id_group = pd.concat(patent_list2)\n",
    "# del patent_list2  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fname = drct+'patent_id_group'\n",
    "# with open(fname, 'wb') as f:\n",
    "#     pickle.dump(patent_id_group, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = drct+'patent_id_group'\n",
    "with open(fname, 'rb') as f:\n",
    "    patent_id_group = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(patent_id_group.patent_id.unique()), len(patent_id_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patent_id_group = patent_id_group.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(patent_id_group.drop_duplicates().patent_id.unique()), len(patent_id_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tmp = patent_id_year.loc[patent_id_year.citation_id.isin(patent_id_group.patent_id.astype(str))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patent_id_group.group_id.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__NOTE:__\n",
    "D in front of patent id indicates \"design\" patent, in contrast to \"utility\" patent.\n",
    "\n",
    "In general terms, a “utility patent” protects the way an article is used and works (35 U.S.C. 101), while a “design patent” protects the way an article looks (35 U.S.C. 171). The ornamental appearance for an article includes its shape/configuration or surface ornamentation applied to the article, or both. Both design and utility patents may be obtained on an article if invention resides both in its utility and ornamental appearance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distinct_groups = df.unique()\n",
    "\n",
    "# fname = drct+'distinct_groups'\n",
    "# with open(fname, 'wb') as f:\n",
    "#     pickle.dump(distinct_groups, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Strategy: Create a (dataframe) table for each year. \n",
    "\n",
    "Each row indicate a patent issued up until now. Followed by the citation vector.\n",
    "Note that the citation vector is updated for each citation each year, and new rows are added (as new patents are issued)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# patent_citation_id_year = pd.DataFrame()\n",
    "# for yr in range(1965,1966):\n",
    "#     fname = drct+'/patent_citation_yr/'+str(yr)+'.pkl'\n",
    "#     with open(fname, 'rb') as f:\n",
    "# #         print(fname)\n",
    "#         df = pickle.load(f)\n",
    "#         patent_citation_id_year = pd.concat([patent_citation_id_year,df])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
